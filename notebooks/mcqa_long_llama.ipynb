{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B-RWSwCTFUx"
      },
      "source": [
        "### 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ6xyR-xS8Zl",
        "outputId": "11e32695-09b5-4ed4-9989-7027fb19dd0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas gdown huggingface-hub numpy matplotlib scikit-learn transformers torch tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImJ21s77uOhW",
        "outputId": "eda3c30c-e7ce-403d-bb46-b0afc18537b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkGeCoTJVDma"
      },
      "source": [
        "### 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_D7g6b6xVGG9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import sys\n",
        "import argparse\n",
        "import re\n",
        "from typing import List, Union\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prJZo492paGc",
        "outputId": "24f3ca01-7a19-447e-e59d-f38cf0f88bdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgcDALLqVGlu"
      },
      "source": [
        "### 3. Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG2LcXVGVKK_",
        "outputId": "310d9a47-e9f2-478a-93b0-7cef5877b522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\",force_remount=True)\n",
        "os.chdir(\"/content/drive/My Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLgpzXE5VLXX"
      },
      "source": [
        "### 4. Utils Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sk6gL82yVNPo"
      },
      "outputs": [],
      "source": [
        "def format_time_difference(seconds):\n",
        "    minutes = seconds // 60\n",
        "    hours = minutes // 60\n",
        "    days = hours // 24\n",
        "\n",
        "    if days > 0:\n",
        "        if hours % 24 > 0.1:\n",
        "            return f\"{days} days-{hours % 24} hours\"\n",
        "        else:\n",
        "            return f\"{days} days\"\n",
        "    elif hours > 0:\n",
        "        if minutes % 60 > 0.1:\n",
        "            return f\"{hours} hours-{minutes % 60} minutes\"\n",
        "        else:\n",
        "            return f\"{hours} hours\"\n",
        "    elif minutes > 0:\n",
        "        if seconds % 60 > 0.1:\n",
        "            return f\"{minutes} minutes-{seconds % 60} seconds\"\n",
        "        else:\n",
        "            return f\"{minutes} minutes\"\n",
        "    else:\n",
        "        return f\"{seconds} seconds\"\n",
        "\n",
        "def save_to_json(data, save_path):\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    with open(save_path, \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "def plot_series(filename, input_ts, output_ts, predicted_ts, save_folder):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(len(input_ts)), input_ts, label=\"Input Time Series\", marker='o')\n",
        "    plt.plot(range(len(input_ts), len(input_ts) + len(output_ts)), output_ts, label=\"Ground Truth\", marker='o')\n",
        "    plt.plot(range(len(input_ts), len(input_ts) + len(predicted_ts)), predicted_ts, label=\"Predicted\", linestyle='dashed')\n",
        "    plt.legend()\n",
        "    plt.title(f\"Prediction for {filename}\")\n",
        "    plt.xlabel(\"Time Steps\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid()\n",
        "    plt.savefig(os.path.join(save_folder, filename.replace('.json', '.png')))\n",
        "    plt.close()\n",
        "\n",
        "def calculate_mape(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    mask = y_true != 0\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "\n",
        "def calculate_acc(result_list, regrouped_labels = None):\n",
        "    if regrouped_labels is None:\n",
        "        correct_pred = sum(1 for result in result_list if result[\"ground_truth\"] in result[\"predict\"])\n",
        "    else:\n",
        "        correct_pred = 0\n",
        "        for result in result_list:\n",
        "            gt_group = regrouped_labels[result['ground_truth']]\n",
        "            for original_label in regrouped_labels.keys():\n",
        "                if original_label in result['predict']:\n",
        "                    predict_group = regrouped_labels[original_label]\n",
        "                    if gt_group == predict_group:\n",
        "                        correct_pred += 1\n",
        "                        break\n",
        "\n",
        "    total_pred = len(result_list)\n",
        "    accuracy = correct_pred / total_pred\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def calculate_correlation_acc(result_list):\n",
        "    model_predictions = {\"total\": 0, \"exact_correct\": 0, \"brief_correct\": 0}\n",
        "    positive_correlations = [\"Strong Positive Correlation\", \"Moderate Positive Correlation\"]\n",
        "    negative_correlations = [\"Strong Negative Correlation\", \"Moderate Negative Correlation\"]\n",
        "    for result in result_list:\n",
        "        prediction = result[\"predict\"].strip()\n",
        "        model_predictions[\"total\"] += 1\n",
        "        if prediction == result[\"ground_truth\"]:\n",
        "            model_predictions[\"exact_correct\"] += 1\n",
        "\n",
        "        # Brief accuracy\n",
        "        pred_is_positive = prediction in positive_correlations\n",
        "        pred_is_negative = prediction in negative_correlations\n",
        "        truth_is_positive = result[\"ground_truth\"] in positive_correlations\n",
        "        truth_is_negative = result[\"ground_truth\"] in negative_correlations\n",
        "\n",
        "        if (pred_is_positive and truth_is_positive) or \\\n",
        "            (pred_is_negative and truth_is_negative) or \\\n",
        "            (prediction == result[\"ground_truth\"]):\n",
        "            model_predictions[\"brief_correct\"] += 1\n",
        "\n",
        "    # Calculate and format results\n",
        "    total = model_predictions[\"total\"]\n",
        "    exact_accuracy = (model_predictions[\"exact_correct\"] / total) * 100\n",
        "    brief_accuracy = (model_predictions[\"brief_correct\"] / total) * 100\n",
        "\n",
        "    metric_results = {\n",
        "        \"exact_accuracy\": f\"{round(exact_accuracy, 2)}%\",\n",
        "        \"brief_accuracy\": f\"{round(brief_accuracy, 2)}%\",\n",
        "        \"total_samples\": total\n",
        "    }\n",
        "    return metric_results\n",
        "\n",
        "\n",
        "def calculate_mcqa_acc(result_list):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for result in result_list:\n",
        "        predition = result[\"predict\"].strip()\n",
        "        predition = predition[0].upper()\n",
        "        if predition == result[\"ground_truth\"]:\n",
        "            correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return accuracy * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eygbt3uqV5bd"
      },
      "source": [
        "### 5. Models Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VzVqjoWMV9ZJ"
      },
      "outputs": [],
      "source": [
        "class BaseModel(ABC):\n",
        "    @abstractmethod\n",
        "    def inference(self, content: str) -> str:\n",
        "        \"\"\"\n",
        "        Run inference on a given input prompt and return the generated output.\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J-7-980UWDSQ"
      },
      "outputs": [],
      "source": [
        "class DeepSeekModel(BaseModel):\n",
        "    def __init__(self, model_name: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", **kwargs):\n",
        "\n",
        "        # The model is set in eval mode by default by using eval()\n",
        "        # See: https://huggingface.co/docs/transformers/en/main_classes/model#transformers.PreTrainedModel\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"auto\",\n",
        "            **kwargs\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def inference(self, content: str) -> str:\n",
        "        messages = [{\"role\": \"user\", \"content\": content}]\n",
        "\n",
        "        chat_prompt = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        tokenized_input = self.tokenizer([chat_prompt], return_tensors=\"pt\").to(self.model.device)\n",
        "        generated_output = self.model.generate(\n",
        "            **tokenized_input,\n",
        "            max_new_tokens=4096,\n",
        "        )\n",
        "        output_ids = generated_output[0][len(tokenized_input.input_ids[0]):].tolist()\n",
        "\n",
        "        # parsing thinking content\n",
        "        try:\n",
        "            # rindex finding 151649 (</think>)\n",
        "            index = len(output_ids) - output_ids[::-1].index(151649)\n",
        "        except ValueError:\n",
        "            index = 0\n",
        "        outputs = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94Rf3ZLIYCuI"
      },
      "outputs": [],
      "source": [
        "class LLaMAModel(BaseModel):\n",
        "    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-1B-Instruct\", **kwargs):\n",
        "        self.pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model_name,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"auto\",\n",
        "            token=\"\",\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def inference(self, content: str) -> str:\n",
        "        messages = [{\"role\": \"user\", \"content\": content}]\n",
        "\n",
        "        outputs = self.pipeline(messages, max_new_tokens=1024)\n",
        "\n",
        "        return outputs[0][\"generated_text\"][-1][\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fdC6qVPZYJVs"
      },
      "outputs": [],
      "source": [
        "class ModelFactory:\n",
        "    def __init__(self, config: dict):\n",
        "        self.config = config\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model(model_type: str, model_name: str, **kwargs) -> BaseModel:\n",
        "        if model_type == \"deepseek\":\n",
        "            return DeepSeekModel(model_name=model_name, **kwargs)\n",
        "        elif model_type == \"llama\":\n",
        "            return LLaMAModel(model_name=model_name, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {model_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsKClRUpYK8m"
      },
      "source": [
        "### 6. Experiment Code --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3Hw8kUS5sd-u"
      },
      "outputs": [],
      "source": [
        "def finance_mse_metaprompt_generation(\n",
        "    text: str,\n",
        "    prices: List[float],\n",
        "    start_datetime: str,\n",
        "    end_datetime: str,\n",
        "    pred_end_datetime: str,\n",
        "    granularity: str,\n",
        "    prediction_length: int,\n",
        "    mode: str,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates a meta-prompt for hypothetical stock price trend analysis\n",
        "    based on given inputs.\n",
        "\n",
        "    Args:\n",
        "        text (str): News article content within the input time series range.\n",
        "        prices (List[float]): Historical stock prices.\n",
        "        start_datetime (str): Start datetime of the input time series.\n",
        "        end_datetime (str): End datetime of the input time series.\n",
        "        pred_end_datetime (str): End datetime of the hypothetical projection.\n",
        "        granularity (str): Granularity of the input time series (e.g., daily, hourly).\n",
        "        prediction_length (int): Number of future time steps to estimate.\n",
        "        mode (str): Mode of estimation (\"timeseries_only\", \"text_only\", \"combined\").\n",
        "\n",
        "    Returns:\n",
        "        str: Meta-prompt for ChatGPT.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        f\"You are an AI assistant trained in data analysis and modeling. \"\n",
        "        f\"Your task is to conduct a research-based timeseries estimation for the next {prediction_length} time steps \"\n",
        "        f\"based on provided historical price movements and/or related news articles. \"\n",
        "        f\"This analysis aims to explore patterns in the given dataset and should not be considered financial advice. \"\n",
        "        f\"The input time series spans from {start_datetime} to {end_datetime}, with a granularity of {granularity}. \"\n",
        "        f\"The estimation period extends from {end_datetime} to {pred_end_datetime}, maintaining the same granularity.\"\n",
        "    )\n",
        "\n",
        "    if mode == \"timeseries_only\":\n",
        "        prompt += (\n",
        "            \"You will analyze the numerical patterns in historical prices and extrapolate potential movements. \"\n",
        "            f\"The input prices are: {prices}. \"\n",
        "        )\n",
        "    elif mode == \"text_only\":\n",
        "        prompt += (\n",
        "            \"You will analyze sentiment and potential market impacts from the following news article content: \"\n",
        "            f\"{text}. \"\n",
        "        )\n",
        "    elif mode == \"combined\":\n",
        "        prompt += (\n",
        "            \"You will use both historical price movements and relevant news sentiment analysis \"\n",
        "            f\"to explore hypothetical market trends. The input prices are: {prices}. The news article states: {text}. \"\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Invalid mode. Choose from 'timeseries_only', 'text_only', or 'combined'.\"\n",
        "        )\n",
        "\n",
        "    prompt += (\n",
        "        \"\\n\\nPlease return your estimated values in a structured format as a  list of float numbers. \"\n",
        "        \"Ensure the output follows this format strictly: \"\n",
        "        \"\\nPredicted Prices: value1, value2, ..., valueN. \"\n",
        "        f\"The number of estimated values should be exactly {prediction_length}. \"\n",
        "    )\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def finance_macd_metaprompt_generation(\n",
        "    text: str,\n",
        "    prices: List[float],\n",
        "    start_datetime: str,\n",
        "    end_datetime: str,\n",
        "    pred_end_datetime: str,\n",
        "    granularity: str,\n",
        "    prediction_length: int,\n",
        "    mode: str,\n",
        ") -> str:\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are an AI assistant trained in data analysis and modeling. \"\n",
        "        f\"Your task is to Predict the future Moving Average Convergence Divergence (MACD) values for the next {prediction_length} time steps \"\n",
        "        f\"based on provided historical timeseries movements and/or related news articles. \"\n",
        "        # f\"This analysis aims to explore patterns in the given dataset and should not be considered financial advice. \"\n",
        "        f\"The input time series spans from {start_datetime} to {end_datetime}, with a granularity of {granularity}. \"\n",
        "        f\"The estimation period extends from {end_datetime} to {pred_end_datetime}, maintaining the same granularity.\"\n",
        "    )\n",
        "\n",
        "    if mode == \"timeseries_only\":\n",
        "        prompt += (\n",
        "            \"You will analyze the numerical patterns in historical prices. \"\n",
        "            f\"The input prices are: {prices}. \"\n",
        "        )\n",
        "    elif mode == \"text_only\":\n",
        "        prompt += (\n",
        "            \"You will analyze sentiment and potential market impacts from the following news article content: \"\n",
        "            f\"{text}. \"\n",
        "        )\n",
        "    elif mode == \"combined\":\n",
        "        prompt += (\n",
        "            \"You will use both historical price movements and relevant text sentiment analysis \"\n",
        "            f\"The input prices are: {prices}. The news article states: {text}. \"\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Invalid mode. Choose from 'timeseries_only', 'text_only', or 'combined'.\"\n",
        "        )\n",
        "\n",
        "    prompt += (\n",
        "        \"\\n\\nPlease return your predicted MACD values in a structured format as a list of float numbers. Please predict the real possible values, do not use the naive linear extrapolation or similar methods\"\n",
        "        \"Ensure the output follows this format strictly: \"\n",
        "        \"\\nPredicted Prices: value1, value2, ..., valueN. \"\n",
        "        f\"The number of predicted values should be exactly {prediction_length}. \"\n",
        "    )\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def finance_bb_metaprompt_generation(\n",
        "    text: str,\n",
        "    prices: List[float],\n",
        "    start_datetime: str,\n",
        "    end_datetime: str,\n",
        "    pred_end_datetime: str,\n",
        "    granularity: str,\n",
        "    prediction_length: int,\n",
        "    mode: str,\n",
        ") -> str:\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are an AI assistant trained in data analysis and modeling. \"\n",
        "        f\"Your task is to Predict the future upper Bollinger Band (BB) values  for the next {prediction_length} time steps \"\n",
        "        f\"based on provided historical price movements and/or related news articles. \"\n",
        "        # f\"This analysis aims to explore patterns in the given dataset and should not be considered financial advice. \"\n",
        "        f\"The input time series spans from {start_datetime} to {end_datetime}, with a granularity of {granularity}. \"\n",
        "        f\"The estimation period extends from {end_datetime} to {pred_end_datetime}, maintaining the same granularity.\"\n",
        "    )\n",
        "\n",
        "    if mode == \"timeseries_only\":\n",
        "        prompt += (\n",
        "            \"You will analyze the numerical patterns in historical prices. \"\n",
        "            f\"The input prices are: {prices}. \"\n",
        "        )\n",
        "    elif mode == \"text_only\":\n",
        "        prompt += (\n",
        "            \"You will analyze sentiment and potential market impacts from the following news article content: \"\n",
        "            f\"{text}. \"\n",
        "        )\n",
        "    elif mode == \"combined\":\n",
        "        prompt += (\n",
        "            \"You will use both historical price movements and relevant news sentiment analysis \"\n",
        "            f\"to explore hypothetical market trends. The input prices are: {prices}. The news article states: {text}. \"\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Invalid mode. Choose from 'timeseries_only', 'text_only', or 'combined'.\"\n",
        "        )\n",
        "\n",
        "    prompt += (\n",
        "        \"\\n\\nPlease return your estimated upper Bollinger Band (BB) values values in a structured format as a list of float numbers. \"\n",
        "        \"Ensure the output follows this format strictly: \"\n",
        "        \"\\nPredicted Prices: value1, value2, ..., valueN. \"\n",
        "        f\"The number of estimated values should be exactly {prediction_length}. \"\n",
        "    )\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def parse_val_prediction_response(response: str) -> Union[List[float], None]:\n",
        "    \"\"\"\n",
        "    Decodes the predicted prices from a response string.\n",
        "\n",
        "    Args:\n",
        "        response (str): The response containing the predicted prices.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: A list of float numbers extracted from the response.\n",
        "        None: If extraction fails.\n",
        "    \"\"\"\n",
        "    match = re.search(r\"Predicted Prices:\\s*([-\\d.,\\s]+)\", response)\n",
        "\n",
        "    if match:\n",
        "        try:\n",
        "            price_list = [float(value) for value in match.group(1).split(',')]\n",
        "            return price_list\n",
        "        except ValueError:\n",
        "            pass  # If conversion fails, try another approach\n",
        "\n",
        "    # Alternative approach: Find all potential numbers in the response\n",
        "    possible_numbers = re.findall(r\"-?\\d+\\.\\d+\", response)\n",
        "    if possible_numbers:\n",
        "        try:\n",
        "            return [float(num) for num in possible_numbers]\n",
        "        except ValueError:\n",
        "            pass  # If conversion fails, return None\n",
        "\n",
        "    return None  # Return None if extraction fails\n",
        "\n",
        "def finance_classification_metaprompt_generation(text=None, timestamps=None, prices=None, mode=None):\n",
        "    time_series_data = \", \".join([f\"{price}\" for price in  prices])\n",
        "\n",
        "    if mode == \"combined\":\n",
        "        meta_prompt = f\"\"\"\n",
        "            You are a financial prediction expert with knowledge of advanced machine learning models and time-series analysis.\n",
        "            Your goal is to predict the stock trend (rise, neutral, or fall) based on the following inputs:\n",
        "\n",
        "            1. **Time Series Stock Price Data**:\n",
        "            - This data includes stock prices recorded at 1-hour intervals over the last month from {timestamps[0]} to {timestamps[-1]}.\n",
        "            - Example data format:\n",
        "                {time_series_data}\n",
        "\n",
        "            2. **News Data**:\n",
        "            - This includes news headlines and summaries relevant to the stock's company or sector.\n",
        "            - Example data format:\n",
        "                {text}\n",
        "\n",
        "            ### Task:\n",
        "            Analyze the provided time-series data and news to identify future trends of the stock performance. Ensure that the news data is used to supplement the insights from the time-series analysis, focusing on combining both inputs for a more accurate prediction.\n",
        "\n",
        "            ### Output:\n",
        "            Provide a prediction for the stock trend categorized one of the following labels:\n",
        "            - \"<-4%\"\n",
        "            - \"-2% ~ -4%\"\n",
        "            - \"-2% ~ +2%\"\n",
        "            - \"+2% ~ +4%\"\n",
        "            - \">+4%\"\n",
        "\n",
        "            please think step-by-step and briefly explain how the combination of time-series data and news data led to the prediction;\n",
        "            then wrap your final answer in the final predicted label in the format ^^^label^^^\n",
        "        \"\"\"\n",
        "\n",
        "    elif mode == \"text_only\":\n",
        "        meta_prompt = f\"\"\"\n",
        "            You are a financial prediction expert with knowledge of advanced machine learning models and time-series analysis.\n",
        "            Your goal is to predict the stock trend with given labels based on the following input:\n",
        "\n",
        "            **News Data**:\n",
        "            - This includes news headlines and summaries relevant to the stock's company or sector.\n",
        "            - Example data format:\n",
        "                {text}\n",
        "\n",
        "            ### Output:\n",
        "            Provide a prediction for the stock trend categorized one of the following labels:\n",
        "            - \"<-4%\"\n",
        "            - \"-2% ~ -4%\"\n",
        "            - \"-2% ~ +2%\"\n",
        "            - \"+2% ~ +4%\"\n",
        "            - \">+4%\"\n",
        "\n",
        "            ### Task:\n",
        "            Analyze the news semantics to identify trends and patterns that could impact stock performance.\n",
        "            Then wrap your final answer in the final predicted label in the format ^^^label^^^\n",
        "        \"\"\"\n",
        "\n",
        "    elif mode == \"timeseries_only\":\n",
        "        meta_prompt = f\"\"\"\n",
        "            You are a financial prediction expert with knowledge of advanced machine learning models and time-series analysis.\n",
        "            Your goal is to predict the stock trend with given labels based on the following input:\n",
        "\n",
        "            1. **Time Series Stock Price Data**:\n",
        "            - This data includes stock prices recorded at 1-hour intervals over the last month from {timestamps[0]} to {timestamps[-1]}.\n",
        "            - Example data format:\n",
        "                {time_series_data}\n",
        "\n",
        "            ### Output:\n",
        "            Provide a prediction for the stock trend categorized one of the following labels:\n",
        "            - \"<-4%\"\n",
        "            - \"-2% ~ -4%\"\n",
        "            - \"-2% ~ +2%\"\n",
        "            - \"+2% ~ +4%\"\n",
        "            - \">+4%\"\n",
        "\n",
        "            ### Task:\n",
        "            Analyze the provided time-series data to identify trends and patterns that could impact stock performance. Focus solely on the time-series data for making predictions.\n",
        "             then wrap your final answer in the final predicted label in the format ^^^label^^^\n",
        "        \"\"\"\n",
        "\n",
        "    return meta_prompt\n",
        "\n",
        "def parse_cls_response(answer):\n",
        "    try:\n",
        "        return  re.findall(r'\\^\\^\\^(.*?)\\^\\^\\^', answer)[-1]\n",
        "    except:\n",
        "        return  re.findall(r'\\^+(.*?)\\^+', answer)[-1]\n",
        "\n",
        "\n",
        "\n",
        "def finance_correlation_metaprompt_generation(setting, sticker, time1, time2, in_price, news, time_news):\n",
        "\n",
        "    time_interval = \"1 hour\" if setting == \"long\" else \"5 minutes\"\n",
        "\n",
        "    if setting == \"long\":\n",
        "        system_prompt =\"You are an expert in finance and stock market analysis. Based on the given 30-day historical stock price time series and a financial analysis published at the last timestamp of the time series, your task is to predict the correlation between the stock's price fluctuations in the next 7 days and the analysis sentiment (positive correlation indicates that positive analysis leads to price increase and negative analysis leads to price decrease). Take into account external factors or market conditions that might affect stock price movement.\"\n",
        "    else:\n",
        "        system_prompt = \"You are an expert in finance and stock market analysis. Based on the given 7-day historical stock price time series and a financial analysis published at the last timestamp of the time series, your task is to predict the correlation between the stock's price fluctuations in the next 1 day and the analysis sentiment (positive correlation indicates that positive analysis leads to price increase and negative analysis leads to price decrease). Take into account external factors or market conditions that might affect stock price movement.\"\n",
        "    question = \"Return your answer in one of the following without any other words: Strong Positive Correlation, Moderate Positive Correlation, No Correlation, Moderate Negative Correlation, Strong Negative Correlation.\"\n",
        "    query = f\"stock price of {sticker} between {time1} to {time2}, time interval is {time_interval}: \\\n",
        "            {in_price}\\\n",
        "            News published at {time_news}: \\\n",
        "            {news}\\\n",
        "            {question} Answer:\"\n",
        "    prompt = f\"{system_prompt}\\n\\n{query}\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def finance_mcqa_metaprompt_generation(setting, sticker, time1, time2, in_price, news, time_news, question):\n",
        "    time_interval = \"1 hour\" if setting == \"long\" else \"5 minutes\"\n",
        "    if setting  == \"long\":\n",
        "        system_prompt =\"You are an expert in finance and stock market analysis. Your task is to answer the question based on the given 30-day historical stock price time series and a financial analysis published at the last timestamp of the time series. Return your answer only in the letter (A, B, C, or D). \"\n",
        "    else:\n",
        "        system_prompt =\"You are an expert in finance and stock market analysis. Your task is to answer the question based on the given 7-day historical stock price time series and a financial analysis published at the last timestamp of the time series. Return your answer only in the letter (A, B, C, or D). \"\n",
        "    query = f\"stock price of {sticker} between {time1} to {time2}, time interval is {time_interval}: \\\n",
        "            {in_price}\\\n",
        "            News published at {time_news}: \\\n",
        "            {news}\\\n",
        "            Question: {question}. Give your answer in the letter (A, B, C, or D) without any other words. Answer:\"\n",
        "    prompt = f\"{system_prompt}\\n\\n{query}\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "j79ILLkFaWC8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.argv = [\n",
        "    \"script_name\",  # Placeholder for script name (ignored by argparse)\n",
        "    \"--dataset_folder\", \"./MTBench-Test/MTBench_finance_QA_long\",\n",
        "    \"--save_path\", \"./MTBench-Test/deepseek/qa_long\",\n",
        "    \"--model_type\", \"llama\",\n",
        "    \"--model\", \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    \"--setting\", \"long\"\n",
        "]\n",
        "\n",
        "# import sys\n",
        "\n",
        "# sys.argv = [\n",
        "#     \"script_name\",  # Placeholder for script name (ignored by argparse)\n",
        "#     \"--dataset_path\", \"./MTBench-Test/MTBench_finance_aligned_pairs_short/train-00000-of-00001.parquet\",\n",
        "#     \"--save_path\", \"./MTBench-Test/<model>/correlation_short\",\n",
        "#     \"--model_type\", \"llama\",\n",
        "#     \"--model\", \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "#     \"--mode\", \"timeseries_only\"\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "47043cb15130426daeb100c76df2d8db",
            "a6bf15b421f94416b12f10367b66954d",
            "9627a6184115419f9756bb65728ba1ae",
            "c4799e68ecc14684825ff8021e1d1560",
            "744d571bddff457995eee3517f478928",
            "83de6611baf848d2af2c51c2a02f07e2",
            "257be2834e104dbba1d7de62b49697b9",
            "5799a95d2ebf42b4973bee23ffa0ebd5",
            "478da38353374849b2bed6603945d0aa",
            "27aa0ca6db6f4364a9f574fa091e7dfa",
            "d15750d5c58c4216ad7b13322d496ca7",
            "eca3ee0e1dbc45c3a0ad0bf930a3b979",
            "c5ea18f19d0c417aabeb4dcbc85dab2e",
            "39b0db9dd30f4509b200c07a9ab90c7a",
            "bae56bd372914f878da901dc1b6f9c1e",
            "7b6fa4339a984f479d032529cfebe9e2",
            "edffe56813b24c6881307a397c758f12",
            "8ee8053d0bf64d97874c656d22547c53",
            "f83ae0652cc94051a3b768ebe783b92d",
            "a2aaf67964db42a8b40067a10c1631c0",
            "29d0b9a59ec44be9b19dcce4c42e5ade",
            "e51db02f108d4d17a172c49e027d1e7a",
            "0e7d174901874ab782806f3cbdc218cf",
            "3770ca6ca30749c6bb7154bc0f38daff",
            "cd6c208c269e42d8abf69c465fbbc894",
            "a03c734c22d04d1f99f74ea997c71ed0",
            "76f23622354f4cb5b9e2f737003628fb",
            "435ff2fd9de3419a873df70d05e74aec",
            "940c426b56224f90af36fbcf6346c86e",
            "beb5d97e3dac4f55b03011a6c34110a4",
            "5dd48a20babf4e8a9c082cefcdf5ece2",
            "fa7f4caebabe49208917711765351c0e",
            "c51cae176e7549dca08685de3c8d2f2a",
            "389b5beef74d4b3fa1e6b52353a67edf",
            "fa89b6252f714f3b845cfd0394f61720",
            "8b9065a29a0842fe903ffcc31bbb035c",
            "fe94cc6fc9c34833bfbe945a20034901",
            "acb660264cbd4b1981ef130c231ab366",
            "3a8721e429104fc9914397f6842561e1",
            "65db5147ff904ef2912cab82c499630b",
            "1142f5c849954e6cac989c784e4a387f",
            "4766c29b38d44c20a62182c3cbb120dd",
            "44a48ae26fb241709d3599d3a2341ef7",
            "4f56bd8d1b014ab5b8359d061752248e",
            "3109dc8210a0462a9b6e2fef2e9c9e19",
            "76ee0a929f8947829b0d45dbd015155c",
            "69e8d7a1010c490a962855e437171e65",
            "02ed99de52864bc9b78351690dc13527",
            "947130b7bd954befa0ed776f788a8655",
            "66c2a2f9e9bd4935b3c8f02c5d30e885",
            "0c911ae2fc4a4d7493afdb3733b2c0ec",
            "236752401f18451a8a6f5c7b38eba5de",
            "00049c5d525042fc94d70ecb029f3ed0",
            "16909b85ff6b4512b4b9ea88603b2f7c",
            "98f6fad28154454fbc4d124207d85b90",
            "9fe8c0ed12b241ce962e11a8e2699cc3",
            "c25429eae7934ab8826c5ea7885cd68c",
            "c40f1ded923f42c88639cfc72a4101df",
            "21a3e7658b454c79b4779d66fe5c09ef",
            "a3c1dc2560954d36980b51e7113ef744",
            "f51d38e3ff314df69a33af09b067254c",
            "d11d1071950843bab6148b1186b9a060",
            "903cd418c29c4ecd897bcb59f4749b90",
            "e915812763fc40ebbb982edffa48a9db",
            "3c909cf22d1e4108b1eec8e07ec33381",
            "961209945c2a463c9fad2c16b42b0966"
          ]
        },
        "id": "XdWvXn4YreWT",
        "outputId": "90dc79db-56c8-48c1-ed4c-abd63a59b4d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47043cb15130426daeb100c76df2d8db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eca3ee0e1dbc45c3a0ad0bf930a3b979",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e7d174901874ab782806f3cbdc218cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "389b5beef74d4b3fa1e6b52353a67edf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3109dc8210a0462a9b6e2fef2e9c9e19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fe8c0ed12b241ce962e11a8e2699cc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating 516 samples......\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/516 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  0%|          | 1/516 [00:04<42:09,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  0%|          | 2/516 [00:07<31:05,  3.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  1%|          | 3/516 [00:07<17:59,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 17.99 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.39 GiB is free. Process 3195 has 4.35 GiB memory in use. Of the allocated memory 3.65 GiB is allocated by PyTorch, and 587.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 4/516 [00:09<14:47,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  1%|          | 5/516 [00:11<17:55,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  1%|          | 6/516 [00:13<16:30,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  1%|▏         | 7/516 [00:14<15:04,  1.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  2%|▏         | 8/516 [00:16<14:17,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  2%|▏         | 9/516 [00:18<16:01,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  2%|▏         | 10/516 [00:21<17:47,  2.11s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  2%|▏         | 11/516 [00:23<19:04,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  2%|▏         | 12/516 [00:27<21:44,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  3%|▎         | 13/516 [00:29<21:03,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  3%|▎         | 14/516 [00:32<22:10,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  3%|▎         | 15/516 [00:34<20:58,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  3%|▎         | 16/516 [00:38<23:04,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  3%|▎         | 17/516 [00:40<21:48,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  3%|▎         | 18/516 [00:42<21:08,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  4%|▎         | 19/516 [00:45<20:26,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  4%|▍         | 20/516 [00:47<20:44,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  4%|▍         | 21/516 [00:48<17:07,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  4%|▍         | 22/516 [00:51<18:32,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  4%|▍         | 23/516 [00:52<15:54,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  5%|▍         | 24/516 [00:55<18:02,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  5%|▍         | 25/516 [00:58<20:32,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  5%|▌         | 26/516 [01:00<17:48,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  5%|▌         | 27/516 [01:02<17:14,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  5%|▌         | 28/516 [01:04<18:28,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  6%|▌         | 29/516 [01:07<18:53,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  6%|▌         | 30/516 [01:10<20:41,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  6%|▌         | 31/516 [01:12<19:31,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  6%|▌         | 32/516 [01:15<20:46,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  6%|▋         | 33/516 [01:18<22:47,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  7%|▋         | 34/516 [01:20<19:54,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  7%|▋         | 35/516 [01:22<18:08,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  7%|▋         | 36/516 [01:25<20:59,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  7%|▋         | 37/516 [01:28<22:01,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  7%|▋         | 38/516 [01:31<22:56,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  8%|▊         | 39/516 [01:33<20:26,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  8%|▊         | 40/516 [01:34<16:44,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  8%|▊         | 41/516 [01:37<17:44,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  8%|▊         | 42/516 [01:40<19:28,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  8%|▊         | 43/516 [01:43<21:17,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  9%|▊         | 44/516 [01:46<22:02,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  9%|▊         | 45/516 [01:48<18:52,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  9%|▉         | 46/516 [01:49<16:28,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  9%|▉         | 47/516 [01:49<12:00,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 23.37 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.69 GiB is free. Process 3195 has 5.04 GiB memory in use. Of the allocated memory 3.93 GiB is allocated by PyTorch, and 1020.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 48/516 [01:50<10:59,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "  9%|▉         | 49/516 [01:53<14:35,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 10%|▉         | 50/516 [01:57<19:28,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 10%|▉         | 51/516 [02:01<22:08,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 10%|█         | 52/516 [02:05<23:55,  3.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 10%|█         | 53/516 [02:08<25:06,  3.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 10%|█         | 54/516 [02:10<21:26,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 11%|█         | 55/516 [02:10<15:24,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 13.35 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.73 GiB is free. Process 3195 has 4.01 GiB memory in use. Of the allocated memory 3.40 GiB is allocated by PyTorch, and 500.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "An error occurred: CUDA out of memory. Tried to allocate 23.67 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.99 GiB is free. Process 3195 has 4.75 GiB memory in use. Of the allocated memory 3.94 GiB is allocated by PyTorch, and 700.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 57/516 [02:13<13:34,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 11%|█         | 58/516 [02:15<13:25,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 11%|█▏        | 59/516 [02:18<16:59,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 12%|█▏        | 60/516 [02:22<19:29,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 12%|█▏        | 61/516 [02:25<21:31,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 12%|█▏        | 62/516 [02:28<20:49,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 12%|█▏        | 63/516 [02:32<22:55,  3.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 12%|█▏        | 64/516 [02:35<23:05,  3.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 13%|█▎        | 65/516 [02:37<20:15,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 13%|█▎        | 66/516 [02:39<20:00,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 13%|█▎        | 67/516 [02:39<14:42,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 20.08 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.05 GiB is free. Process 3195 has 4.69 GiB memory in use. Of the allocated memory 3.76 GiB is allocated by PyTorch, and 828.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 68/516 [02:42<16:56,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 13%|█▎        | 69/516 [02:44<15:28,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 14%|█▎        | 70/516 [02:47<16:45,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 14%|█▍        | 71/516 [02:51<20:24,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 14%|█▍        | 72/516 [02:54<20:42,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 14%|█▍        | 73/516 [02:56<19:06,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 14%|█▍        | 74/516 [02:58<19:35,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 15%|█▍        | 75/516 [03:02<21:53,  2.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 15%|█▍        | 76/516 [03:02<15:45,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 7.99 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.80 GiB is free. Process 3195 has 11.94 GiB memory in use. Of the allocated memory 11.01 GiB is allocated by PyTorch, and 828.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▍        | 77/516 [03:04<14:28,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 15%|█▌        | 78/516 [03:07<16:17,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 15%|█▌        | 79/516 [03:10<18:53,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 16%|█▌        | 80/516 [03:13<19:52,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 16%|█▌        | 81/516 [03:16<20:22,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 16%|█▌        | 82/516 [03:18<17:41,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 16%|█▌        | 83/516 [03:20<15:54,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 16%|█▋        | 84/516 [03:22<16:38,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 16%|█▋        | 85/516 [03:26<19:12,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 17%|█▋        | 86/516 [03:27<16:35,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 17%|█▋        | 87/516 [03:29<15:29,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 17%|█▋        | 88/516 [03:32<17:19,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 17%|█▋        | 89/516 [03:37<22:10,  3.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 17%|█▋        | 90/516 [03:40<23:11,  3.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 18%|█▊        | 91/516 [03:42<19:03,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 18%|█▊        | 92/516 [03:43<16:10,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 18%|█▊        | 93/516 [03:44<14:21,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 18%|█▊        | 94/516 [03:48<16:52,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 18%|█▊        | 95/516 [03:51<18:36,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 19%|█▊        | 96/516 [03:53<18:21,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 19%|█▉        | 97/516 [03:56<17:48,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 19%|█▉        | 98/516 [03:56<13:03,  1.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 10.21 GiB. GPU 0 has a total capacity of 14.74 GiB of which 602.12 MiB is free. Process 3195 has 14.15 GiB memory in use. Of the allocated memory 13.35 GiB is allocated by PyTorch, and 692.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 19%|█▉        | 99/516 [03:58<13:44,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 19%|█▉        | 100/516 [04:01<14:34,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 20%|█▉        | 101/516 [04:01<10:50,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 9.13 GiB. GPU 0 has a total capacity of 14.74 GiB of which 602.12 MiB is free. Process 3195 has 14.15 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 1.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|█▉        | 102/516 [04:03<10:41,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 20%|█▉        | 103/516 [04:04<10:15,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 20%|██        | 104/516 [04:07<13:13,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 20%|██        | 105/516 [04:10<15:22,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 21%|██        | 106/516 [04:12<14:43,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 21%|██        | 107/516 [04:15<16:11,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 21%|██        | 108/516 [04:18<17:30,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 21%|██        | 109/516 [04:21<18:21,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 21%|██▏       | 110/516 [04:24<20:07,  2.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 22%|██▏       | 111/516 [04:24<14:18,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 24.87 GiB. GPU 0 has a total capacity of 14.74 GiB of which 602.12 MiB is free. Process 3195 has 14.15 GiB memory in use. Of the allocated memory 4.00 GiB is allocated by PyTorch, and 10.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 112/516 [04:27<15:31,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 22%|██▏       | 113/516 [04:29<13:53,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 22%|██▏       | 114/516 [04:31<14:30,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 22%|██▏       | 115/516 [04:35<17:02,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 22%|██▏       | 116/516 [04:38<18:01,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 23%|██▎       | 117/516 [04:40<17:43,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 23%|██▎       | 118/516 [04:43<18:32,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 23%|██▎       | 119/516 [04:43<13:18,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 34.74 GiB. GPU 0 has a total capacity of 14.74 GiB of which 602.12 MiB is free. Process 3195 has 14.15 GiB memory in use. Of the allocated memory 4.47 GiB is allocated by PyTorch, and 9.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 120/516 [04:46<13:36,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 23%|██▎       | 121/516 [04:49<16:55,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 24%|██▎       | 122/516 [04:52<16:01,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 24%|██▍       | 123/516 [04:54<15:12,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 24%|██▍       | 124/516 [04:55<13:56,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 24%|██▍       | 125/516 [04:58<14:38,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 24%|██▍       | 126/516 [04:59<12:55,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 25%|██▍       | 127/516 [05:03<15:35,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 25%|██▍       | 128/516 [05:06<16:45,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 25%|██▌       | 129/516 [05:08<16:51,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 25%|██▌       | 130/516 [05:11<16:14,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 25%|██▌       | 131/516 [05:14<18:33,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 26%|██▌       | 132/516 [05:16<15:59,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 26%|██▌       | 133/516 [05:19<16:46,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 26%|██▌       | 134/516 [05:21<15:33,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 26%|██▌       | 135/516 [05:24<16:17,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 26%|██▋       | 136/516 [05:27<18:06,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 27%|██▋       | 137/516 [05:30<17:52,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 27%|██▋       | 138/516 [05:33<17:21,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 27%|██▋       | 139/516 [05:36<17:46,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 27%|██▋       | 140/516 [05:38<17:16,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 27%|██▋       | 141/516 [05:40<15:13,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 28%|██▊       | 142/516 [05:42<14:01,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 28%|██▊       | 143/516 [05:45<15:28,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 28%|██▊       | 144/516 [05:47<15:37,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 28%|██▊       | 145/516 [05:50<16:30,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 28%|██▊       | 146/516 [05:54<17:29,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 28%|██▊       | 147/516 [05:55<14:52,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 29%|██▊       | 148/516 [05:58<16:28,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 29%|██▉       | 149/516 [06:00<13:59,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 29%|██▉       | 150/516 [06:01<12:22,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 29%|██▉       | 151/516 [06:05<14:55,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 29%|██▉       | 152/516 [06:06<13:11,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 30%|██▉       | 153/516 [06:06<09:27,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 29.79 GiB. GPU 0 has a total capacity of 14.74 GiB of which 602.12 MiB is free. Process 3195 has 14.15 GiB memory in use. Of the allocated memory 4.24 GiB is allocated by PyTorch, and 9.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|██▉       | 154/516 [06:08<09:29,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 30%|███       | 155/516 [06:10<10:48,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 30%|███       | 156/516 [06:14<14:06,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 30%|███       | 157/516 [06:17<15:09,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 31%|███       | 158/516 [06:20<16:03,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 31%|███       | 159/516 [06:23<16:09,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 31%|███       | 160/516 [06:25<16:23,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 31%|███       | 161/516 [06:28<16:53,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 31%|███▏      | 162/516 [06:30<14:42,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 32%|███▏      | 163/516 [06:34<16:42,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 32%|███▏      | 164/516 [06:36<16:10,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 32%|███▏      | 165/516 [06:38<14:08,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 32%|███▏      | 166/516 [06:41<15:02,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 32%|███▏      | 167/516 [06:44<15:18,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 33%|███▎      | 168/516 [06:46<13:56,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 33%|███▎      | 169/516 [06:49<14:59,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 33%|███▎      | 170/516 [06:51<15:03,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 33%|███▎      | 171/516 [06:54<15:37,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 33%|███▎      | 172/516 [06:56<14:07,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 34%|███▎      | 173/516 [06:59<15:04,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 34%|███▎      | 174/516 [07:03<17:42,  3.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 34%|███▍      | 175/516 [07:06<17:08,  3.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 34%|███▍      | 176/516 [07:09<17:09,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 34%|███▍      | 177/516 [07:13<18:34,  3.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 34%|███▍      | 178/516 [07:17<18:53,  3.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 35%|███▍      | 179/516 [07:18<15:12,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 35%|███▍      | 180/516 [07:21<15:18,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 35%|███▌      | 181/516 [07:21<11:09,  2.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 1.24 GiB. GPU 0 has a total capacity of 14.74 GiB of which 88.12 MiB is free. Process 3195 has 14.65 GiB memory in use. Of the allocated memory 12.89 GiB is allocated by PyTorch, and 1.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 35%|███▌      | 182/516 [07:29<21:20,  3.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 35%|███▌      | 183/516 [07:31<17:40,  3.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 36%|███▌      | 184/516 [07:34<17:29,  3.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 36%|███▌      | 185/516 [07:37<18:22,  3.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 36%|███▌      | 186/516 [07:41<18:49,  3.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 36%|███▌      | 187/516 [07:44<17:54,  3.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 36%|███▋      | 188/516 [07:47<16:37,  3.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 37%|███▋      | 189/516 [07:49<16:06,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 37%|███▋      | 190/516 [07:52<15:19,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 37%|███▋      | 191/516 [07:55<16:07,  2.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 37%|███▋      | 192/516 [07:58<16:21,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 37%|███▋      | 193/516 [08:01<15:00,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 38%|███▊      | 194/516 [08:02<12:24,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 38%|███▊      | 195/516 [08:05<13:24,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 38%|███▊      | 196/516 [08:07<13:29,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 38%|███▊      | 197/516 [08:09<12:32,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 38%|███▊      | 198/516 [08:12<12:26,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 39%|███▊      | 199/516 [08:18<19:19,  3.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 39%|███▉      | 200/516 [08:21<18:11,  3.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 39%|███▉      | 201/516 [08:23<15:40,  2.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 39%|███▉      | 202/516 [08:25<13:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 39%|███▉      | 203/516 [08:25<10:11,  1.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 22.66 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.30 GiB is free. Process 3195 has 4.44 GiB memory in use. Of the allocated memory 3.89 GiB is allocated by PyTorch, and 441.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 204/516 [08:27<10:02,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 40%|███▉      | 205/516 [08:30<11:36,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 40%|███▉      | 206/516 [08:32<10:23,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 40%|████      | 207/516 [08:34<10:55,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 40%|████      | 208/516 [08:36<11:09,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 41%|████      | 209/516 [08:38<10:34,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 41%|████      | 210/516 [08:40<10:39,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 41%|████      | 211/516 [08:43<12:15,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 41%|████      | 212/516 [08:47<14:14,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 41%|████▏     | 213/516 [08:50<14:00,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 41%|████▏     | 214/516 [08:53<14:08,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 42%|████▏     | 215/516 [08:53<10:08,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 18.95 GiB. GPU 0 has a total capacity of 14.74 GiB of which 9.94 GiB is free. Process 3195 has 4.80 GiB memory in use. Of the allocated memory 3.70 GiB is allocated by PyTorch, and 1000.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 216/516 [08:56<11:38,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 42%|████▏     | 217/516 [08:58<11:52,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 42%|████▏     | 218/516 [09:00<10:51,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 42%|████▏     | 219/516 [09:03<12:09,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 43%|████▎     | 220/516 [09:06<12:05,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 43%|████▎     | 221/516 [09:08<12:18,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 43%|████▎     | 222/516 [09:11<12:22,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 43%|████▎     | 223/516 [09:15<13:56,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 43%|████▎     | 224/516 [09:18<14:55,  3.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 44%|████▎     | 225/516 [09:21<13:54,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 44%|████▍     | 226/516 [09:23<13:27,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 44%|████▍     | 227/516 [09:26<13:40,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 44%|████▍     | 228/516 [09:29<13:21,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 44%|████▍     | 229/516 [09:31<12:11,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 45%|████▍     | 230/516 [09:34<12:53,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 45%|████▍     | 231/516 [09:36<12:47,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 45%|████▍     | 232/516 [09:39<12:58,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 45%|████▌     | 233/516 [09:42<12:54,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 45%|████▌     | 234/516 [09:46<14:10,  3.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 46%|████▌     | 235/516 [09:48<12:44,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 46%|████▌     | 236/516 [09:50<12:13,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 46%|████▌     | 237/516 [09:53<12:37,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 46%|████▌     | 238/516 [09:56<12:47,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 46%|████▋     | 239/516 [10:00<14:02,  3.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 47%|████▋     | 240/516 [10:03<13:51,  3.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 47%|████▋     | 241/516 [10:05<12:47,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 47%|████▋     | 242/516 [10:09<14:27,  3.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 47%|████▋     | 243/516 [10:12<14:22,  3.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 47%|████▋     | 244/516 [10:15<14:15,  3.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 47%|████▋     | 245/516 [10:17<12:42,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 48%|████▊     | 246/516 [10:18<10:35,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 48%|████▊     | 247/516 [10:22<12:16,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 48%|████▊     | 248/516 [10:25<12:00,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 48%|████▊     | 249/516 [10:27<11:19,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 48%|████▊     | 250/516 [10:29<11:20,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 49%|████▊     | 251/516 [10:31<10:32,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 49%|████▉     | 252/516 [10:35<11:45,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 49%|████▉     | 253/516 [10:37<10:54,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 49%|████▉     | 254/516 [10:40<11:08,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 49%|████▉     | 255/516 [10:42<11:12,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 50%|████▉     | 256/516 [10:44<09:59,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 50%|████▉     | 257/516 [10:46<10:12,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 50%|█████     | 258/516 [10:49<10:49,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 50%|█████     | 259/516 [10:52<10:45,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 50%|█████     | 260/516 [10:54<09:51,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 51%|█████     | 261/516 [10:56<09:42,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 51%|█████     | 262/516 [10:58<10:12,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 51%|█████     | 263/516 [11:00<09:23,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 51%|█████     | 264/516 [11:03<10:16,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 51%|█████▏    | 265/516 [11:05<08:55,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 52%|█████▏    | 266/516 [11:05<06:35,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 6.36 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 9.27 GiB is allocated by PyTorch, and 173.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 267/516 [11:08<08:03,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 52%|█████▏    | 268/516 [11:10<08:45,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 52%|█████▏    | 269/516 [11:12<07:56,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 52%|█████▏    | 270/516 [11:14<07:54,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 53%|█████▎    | 271/516 [11:15<07:14,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 53%|█████▎    | 272/516 [11:18<08:54,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 53%|█████▎    | 273/516 [11:20<07:48,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 53%|█████▎    | 274/516 [11:21<07:36,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 53%|█████▎    | 275/516 [11:24<08:44,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 53%|█████▎    | 276/516 [11:27<09:46,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 54%|█████▎    | 277/516 [11:31<10:41,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 54%|█████▍    | 278/516 [11:33<10:12,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 54%|█████▍    | 279/516 [11:36<10:22,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 54%|█████▍    | 280/516 [11:38<10:33,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 54%|█████▍    | 281/516 [11:41<10:36,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 55%|█████▍    | 282/516 [11:44<10:26,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 55%|█████▍    | 283/516 [11:48<12:23,  3.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 55%|█████▌    | 284/516 [11:50<10:32,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 55%|█████▌    | 285/516 [11:53<10:52,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 55%|█████▌    | 286/516 [11:56<11:19,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 56%|█████▌    | 287/516 [11:58<10:20,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 56%|█████▌    | 288/516 [12:00<09:06,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 56%|█████▌    | 289/516 [12:03<09:41,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 56%|█████▌    | 290/516 [12:06<10:19,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 56%|█████▋    | 291/516 [12:09<10:40,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 57%|█████▋    | 292/516 [12:12<10:47,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 57%|█████▋    | 293/516 [12:15<11:14,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 57%|█████▋    | 294/516 [12:16<07:57,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 17.25 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 5.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 295/516 [12:19<09:25,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 57%|█████▋    | 296/516 [12:23<10:39,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 58%|█████▊    | 297/516 [12:26<11:17,  3.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 58%|█████▊    | 298/516 [12:30<11:20,  3.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 58%|█████▊    | 299/516 [12:31<09:31,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 58%|█████▊    | 300/516 [12:34<09:50,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 58%|█████▊    | 301/516 [12:36<08:40,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 59%|█████▊    | 302/516 [12:37<07:48,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 59%|█████▊    | 303/516 [12:42<10:16,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 59%|█████▉    | 304/516 [12:45<10:38,  3.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 59%|█████▉    | 305/516 [12:47<09:50,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 59%|█████▉    | 306/516 [12:49<08:47,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 59%|█████▉    | 307/516 [12:51<07:39,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 60%|█████▉    | 308/516 [12:54<08:44,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 60%|█████▉    | 309/516 [12:55<07:31,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 60%|██████    | 310/516 [12:57<06:50,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 60%|██████    | 311/516 [12:58<06:09,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 60%|██████    | 312/516 [13:02<07:53,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 61%|██████    | 313/516 [13:05<08:33,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 61%|██████    | 314/516 [13:08<09:03,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 61%|██████    | 315/516 [13:09<07:38,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 61%|██████    | 316/516 [13:12<08:01,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 61%|██████▏   | 317/516 [13:13<07:01,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 62%|██████▏   | 318/516 [13:14<05:04,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 48.36 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 5.09 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 319/516 [13:17<06:55,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 62%|██████▏   | 320/516 [13:20<08:12,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 62%|██████▏   | 321/516 [13:24<09:22,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 62%|██████▏   | 322/516 [13:28<10:01,  3.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 63%|██████▎   | 323/516 [13:31<09:37,  2.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 63%|██████▎   | 324/516 [13:34<09:44,  3.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 63%|██████▎   | 325/516 [13:37<10:04,  3.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 63%|██████▎   | 326/516 [13:40<09:43,  3.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 63%|██████▎   | 327/516 [13:43<09:11,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 64%|██████▎   | 328/516 [13:46<09:41,  3.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 64%|██████▍   | 329/516 [13:48<08:39,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 64%|██████▍   | 330/516 [13:50<07:34,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 64%|██████▍   | 331/516 [13:53<08:18,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 64%|██████▍   | 332/516 [13:56<08:15,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 65%|██████▍   | 333/516 [13:58<07:19,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 65%|██████▍   | 334/516 [14:01<08:19,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 65%|██████▍   | 335/516 [14:02<07:04,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 65%|██████▌   | 336/516 [14:04<06:42,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 65%|██████▌   | 337/516 [14:08<07:25,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 66%|██████▌   | 338/516 [14:11<08:24,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 66%|██████▌   | 339/516 [14:17<11:08,  3.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 66%|██████▌   | 340/516 [14:19<09:43,  3.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 66%|██████▌   | 341/516 [14:21<08:08,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 66%|██████▋   | 342/516 [14:24<07:59,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 66%|██████▋   | 343/516 [14:24<05:40,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 26.31 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 4.07 GiB is allocated by PyTorch, and 5.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 344/516 [14:25<05:16,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 67%|██████▋   | 345/516 [14:28<06:12,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 67%|██████▋   | 346/516 [14:31<06:21,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 67%|██████▋   | 347/516 [14:33<06:21,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 67%|██████▋   | 348/516 [14:36<07:01,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 68%|██████▊   | 349/516 [14:39<07:04,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 68%|██████▊   | 350/516 [14:42<07:26,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 68%|██████▊   | 351/516 [14:45<08:09,  2.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 14.10 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 3.44 GiB is allocated by PyTorch, and 6.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 353/516 [14:48<05:57,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 69%|██████▊   | 354/516 [14:52<07:28,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 69%|██████▉   | 355/516 [14:54<06:23,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 69%|██████▉   | 356/516 [14:57<06:50,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 69%|██████▉   | 357/516 [14:58<05:41,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 8.42 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 3.10 GiB is allocated by PyTorch, and 6.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|██████▉   | 359/516 [15:01<04:49,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 70%|██████▉   | 360/516 [15:03<05:08,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 3.22 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 362/516 [15:05<03:49,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 70%|███████   | 363/516 [15:09<05:16,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 71%|███████   | 364/516 [15:12<05:40,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 71%|███████   | 365/516 [15:15<06:36,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 71%|███████   | 366/516 [15:18<06:46,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 71%|███████   | 367/516 [15:21<06:54,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 71%|███████▏  | 368/516 [15:24<07:03,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 72%|███████▏  | 369/516 [15:27<07:08,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 72%|███████▏  | 370/516 [15:27<05:05,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 17.35 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 5.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 371/516 [15:29<04:42,  1.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 72%|███████▏  | 372/516 [15:31<04:35,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 72%|███████▏  | 373/516 [15:34<05:17,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 72%|███████▏  | 374/516 [15:36<05:09,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 73%|███████▎  | 375/516 [15:40<06:12,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 73%|███████▎  | 376/516 [15:42<05:56,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 73%|███████▎  | 377/516 [15:44<05:36,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 73%|███████▎  | 378/516 [15:48<06:48,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 73%|███████▎  | 379/516 [15:51<06:54,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 74%|███████▎  | 380/516 [15:55<06:54,  3.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 74%|███████▍  | 381/516 [15:58<06:50,  3.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 74%|███████▍  | 382/516 [16:00<06:22,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 74%|███████▍  | 383/516 [16:03<06:29,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 74%|███████▍  | 384/516 [16:06<06:20,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 75%|███████▍  | 385/516 [16:07<05:07,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 75%|███████▍  | 386/516 [16:10<05:30,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 75%|███████▌  | 387/516 [16:12<05:23,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 75%|███████▌  | 388/516 [16:14<04:39,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 75%|███████▌  | 389/516 [16:16<04:21,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 76%|███████▌  | 390/516 [16:17<03:52,  1.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 76%|███████▌  | 391/516 [16:18<03:25,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 76%|███████▌  | 392/516 [16:22<04:32,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 76%|███████▌  | 393/516 [16:25<05:01,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 76%|███████▋  | 394/516 [16:28<05:18,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 77%|███████▋  | 395/516 [16:31<05:39,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 77%|███████▋  | 396/516 [16:31<03:59,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 21.08 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 3.81 GiB is allocated by PyTorch, and 5.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 397/516 [16:34<04:22,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 77%|███████▋  | 398/516 [16:37<04:57,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 77%|███████▋  | 399/516 [16:38<04:03,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 78%|███████▊  | 400/516 [16:42<04:52,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 78%|███████▊  | 401/516 [16:46<05:43,  2.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 78%|███████▊  | 402/516 [16:48<05:20,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 78%|███████▊  | 403/516 [16:51<05:22,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 78%|███████▊  | 404/516 [16:53<04:59,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 78%|███████▊  | 405/516 [16:55<04:39,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 79%|███████▊  | 406/516 [16:58<04:51,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 79%|███████▉  | 407/516 [17:01<04:56,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 79%|███████▉  | 408/516 [17:05<05:16,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 79%|███████▉  | 409/516 [17:08<05:40,  3.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 79%|███████▉  | 410/516 [17:11<05:25,  3.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 80%|███████▉  | 411/516 [17:14<04:59,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 80%|███████▉  | 412/516 [17:16<04:53,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 80%|████████  | 413/516 [17:20<05:01,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 80%|████████  | 414/516 [17:23<05:00,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 80%|████████  | 415/516 [17:23<03:32,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 23.59 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 3.94 GiB is allocated by PyTorch, and 5.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 416/516 [17:26<03:55,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 81%|████████  | 417/516 [17:28<03:40,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 81%|████████  | 418/516 [17:30<03:39,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 81%|████████  | 419/516 [17:34<04:22,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 81%|████████▏ | 420/516 [17:36<04:07,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 82%|████████▏ | 421/516 [17:39<04:24,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 82%|████████▏ | 422/516 [17:42<04:34,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 82%|████████▏ | 423/516 [17:45<04:32,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 82%|████████▏ | 424/516 [17:48<04:28,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 82%|████████▏ | 425/516 [17:51<04:33,  3.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 83%|████████▎ | 426/516 [17:54<04:21,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 83%|████████▎ | 427/516 [17:55<03:38,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 83%|████████▎ | 428/516 [18:00<04:22,  2.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 83%|████████▎ | 429/516 [18:06<05:55,  4.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 83%|████████▎ | 430/516 [18:09<05:22,  3.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 84%|████████▎ | 431/516 [18:11<04:17,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 84%|████████▎ | 432/516 [18:13<03:56,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 84%|████████▍ | 433/516 [18:15<03:36,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 84%|████████▍ | 434/516 [18:18<03:42,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 84%|████████▍ | 435/516 [18:19<03:07,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 84%|████████▍ | 436/516 [18:21<02:41,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 85%|████████▍ | 437/516 [18:23<02:52,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 85%|████████▍ | 438/516 [18:26<03:00,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 85%|████████▌ | 439/516 [18:27<02:37,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 85%|████████▌ | 440/516 [18:30<02:42,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 85%|████████▌ | 441/516 [18:31<02:20,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 86%|████████▌ | 442/516 [18:31<01:39,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 20.07 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 3.76 GiB is allocated by PyTorch, and 5.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 443/516 [18:34<02:19,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 86%|████████▌ | 444/516 [18:37<02:40,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 86%|████████▌ | 445/516 [18:39<02:22,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 86%|████████▋ | 446/516 [18:44<03:23,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 87%|████████▋ | 447/516 [18:45<02:53,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 87%|████████▋ | 448/516 [18:49<03:13,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 87%|████████▋ | 449/516 [18:52<03:12,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 87%|████████▋ | 450/516 [18:55<03:13,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 87%|████████▋ | 451/516 [18:58<03:13,  2.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 88%|████████▊ | 452/516 [19:01<03:00,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 88%|████████▊ | 453/516 [19:03<02:53,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 88%|████████▊ | 454/516 [19:05<02:27,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 88%|████████▊ | 455/516 [19:07<02:20,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 88%|████████▊ | 456/516 [19:10<02:28,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 89%|████████▊ | 457/516 [19:13<02:39,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 89%|████████▉ | 458/516 [19:14<02:13,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 89%|████████▉ | 459/516 [19:17<02:25,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 89%|████████▉ | 460/516 [19:20<02:22,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 89%|████████▉ | 461/516 [19:25<02:55,  3.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 90%|████████▉ | 462/516 [19:26<02:22,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 90%|████████▉ | 463/516 [19:29<02:28,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 90%|████████▉ | 464/516 [19:31<02:13,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 90%|█████████ | 465/516 [19:34<02:15,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 90%|█████████ | 466/516 [19:37<02:14,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 91%|█████████ | 467/516 [19:37<01:34,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 39.60 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.18 GiB is free. Process 3195 has 9.56 GiB memory in use. Of the allocated memory 4.70 GiB is allocated by PyTorch, and 4.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 468/516 [19:40<01:50,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 91%|█████████ | 469/516 [19:43<01:56,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 91%|█████████ | 470/516 [19:47<02:13,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 91%|█████████▏| 471/516 [19:51<02:21,  3.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 91%|█████████▏| 472/516 [19:58<03:13,  4.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 92%|█████████▏| 473/516 [20:01<02:53,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 92%|█████████▏| 474/516 [20:04<02:32,  3.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 92%|█████████▏| 475/516 [20:07<02:19,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 92%|█████████▏| 476/516 [20:10<02:15,  3.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 92%|█████████▏| 477/516 [20:13<02:03,  3.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 93%|█████████▎| 478/516 [20:13<01:27,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 15.16 GiB. GPU 0 has a total capacity of 14.74 GiB of which 10.81 GiB is free. Process 3195 has 3.92 GiB memory in use. Of the allocated memory 3.50 GiB is allocated by PyTorch, and 311.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 479/516 [20:17<01:39,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 93%|█████████▎| 480/516 [20:19<01:35,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 93%|█████████▎| 481/516 [20:23<01:49,  3.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 93%|█████████▎| 482/516 [20:26<01:40,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 94%|█████████▎| 483/516 [20:27<01:21,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 94%|█████████▍| 484/516 [20:29<01:07,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 94%|█████████▍| 485/516 [20:31<01:12,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 94%|█████████▍| 486/516 [20:41<02:11,  4.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 94%|█████████▍| 487/516 [20:44<01:55,  4.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 95%|█████████▍| 488/516 [20:47<01:43,  3.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 95%|█████████▍| 489/516 [20:48<01:23,  3.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 95%|█████████▍| 490/516 [20:51<01:14,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 95%|█████████▌| 491/516 [20:52<01:00,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 95%|█████████▌| 492/516 [20:54<00:52,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 96%|█████████▌| 493/516 [20:56<00:50,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 96%|█████████▌| 494/516 [21:00<00:57,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 96%|█████████▌| 495/516 [21:03<01:00,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 96%|█████████▌| 496/516 [21:06<00:58,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 96%|█████████▋| 497/516 [21:10<01:04,  3.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 97%|█████████▋| 498/516 [21:13<00:58,  3.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 97%|█████████▋| 499/516 [21:17<00:57,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 97%|█████████▋| 500/516 [21:19<00:46,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 97%|█████████▋| 501/516 [21:21<00:37,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 97%|█████████▋| 502/516 [21:24<00:37,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 97%|█████████▋| 503/516 [21:26<00:32,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 98%|█████████▊| 504/516 [21:27<00:26,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 98%|█████████▊| 505/516 [21:31<00:29,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 98%|█████████▊| 506/516 [21:33<00:26,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 98%|█████████▊| 507/516 [21:36<00:23,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 98%|█████████▊| 508/516 [21:38<00:20,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 99%|█████████▊| 509/516 [21:46<00:27,  3.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 99%|█████████▉| 510/516 [21:48<00:20,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 99%|█████████▉| 511/516 [21:51<00:16,  3.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 99%|█████████▉| 512/516 [21:53<00:12,  3.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            " 99%|█████████▉| 513/516 [21:56<00:08,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "100%|█████████▉| 514/516 [21:59<00:05,  2.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "100%|█████████▉| 515/516 [22:02<00:02,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "100%|██████████| 516/516 [22:05<00:00,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 31.083844580777097%. Results saved to ./MTBench-Test/deepseek/qa_long/results.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "To run:\n",
        "\n",
        "python src/finance/mcqa.py \\\n",
        "--dataset_folder /path/to/dataset \\\n",
        "--save_path /path/to/save/results \\\n",
        "--model_type deepseek or llama \\\n",
        "--model author/model HuggingFace ID\\\n",
        "--setting short or long\n",
        "\"\"\"\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--dataset_folder\", type=str, help=\"path to the datasets\")\n",
        "parser.add_argument(\"--save_path\", type=str, help=\"path to save the results\")\n",
        "parser.add_argument(\"--model_type\",  type=str, help=\"deepseek or llama\")\n",
        "parser.add_argument(\"--model\",  type=str, help=\"model name\")\n",
        "parser.add_argument(\"--setting\",  type=str, help=\"short or long\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "data_list = []\n",
        "directory_path = Path(args.dataset_folder)\n",
        "for json_file in directory_path.glob(\"*.json\"):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "        sticker = json_file.name.split('_')[1].split('.')[0]\n",
        "        extracted_data = {\n",
        "            \"filename\": json_file.name,\n",
        "            \"sticker\": sticker,\n",
        "            \"index\": int(json_file.name.split('_')[0]),\n",
        "            \"input_timestamps\": data.get(\"input_timestamps\"),\n",
        "            \"input_window\": data.get(\"input_window\"),\n",
        "            \"output_timestamps\": data.get(\"output_timestamps\"),\n",
        "            \"output_window\": data.get(\"output_window\"),\n",
        "            \"question\": data.get('MCQA').get('question'),\n",
        "            \"answer\": data.get('MCQA').get('answer'),\n",
        "            \"text\": data.get(\"text\"),\n",
        "            \"published_utc\": data.get(\"published_utc\")\n",
        "        }\n",
        "        data_list.append(extracted_data)\n",
        "\n",
        "os.makedirs(Path(args.save_path), exist_ok=True)\n",
        "\n",
        "model = ModelFactory.get_model(model_type=args.model_type, model_name=args.model)\n",
        "\n",
        "result_list = []\n",
        "tot_samples = len(data_list)\n",
        "print(\"Evaluating {} samples......\".format(tot_samples))\n",
        "\n",
        "for idx, sample in tqdm(enumerate(data_list), total=tot_samples):\n",
        "    designed_prompt = finance_mcqa_metaprompt_generation(\n",
        "        setting=args.setting,\n",
        "        sticker=sample[\"sticker\"],\n",
        "        time1=datetime.fromtimestamp(sample[\"input_timestamps\"][0]),\n",
        "        time2=datetime.fromtimestamp(sample[\"input_timestamps\"][-1]),\n",
        "        in_price=sample[\"input_window\"],\n",
        "        news=sample[\"text\"],\n",
        "        time_news=sample[\"published_utc\"],\n",
        "        question=sample[\"question\"]\n",
        "    )\n",
        "    try:\n",
        "        answer = model.inference(designed_prompt)\n",
        "        answer = answer.strip().replace('\"', '')\n",
        "        res = {\n",
        "            \"cnt\": len(result_list),\n",
        "            \"filename\": sample[\"filename\"],\n",
        "            \"question\": sample[\"question\"],\n",
        "            \"ground_truth\": sample[\"answer\"],\n",
        "            \"predict\": answer,\n",
        "        }\n",
        "        result_list.append(res)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    if (idx +1) % 20 == 0:\n",
        "        save_to_json(result_list, save_path=f\"{args.save_path}/results.json\")\n",
        "\n",
        "\n",
        "save_to_json(result_list, save_path=f\"{args.save_path}/results.json\")\n",
        "accuracy = calculate_mcqa_acc(result_list)\n",
        "print(f\"Accuracy: {accuracy}%. Results saved to {args.save_path}/results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FCm3r3kFLlMU"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ms2A9buhLmH9"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00049c5d525042fc94d70ecb029f3ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02ed99de52864bc9b78351690dc13527": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16909b85ff6b4512b4b9ea88603b2f7c",
            "placeholder": "​",
            "style": "IPY_MODEL_98f6fad28154454fbc4d124207d85b90",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 14.0MB/s]"
          }
        },
        "0c911ae2fc4a4d7493afdb3733b2c0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e7d174901874ab782806f3cbdc218cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3770ca6ca30749c6bb7154bc0f38daff",
              "IPY_MODEL_cd6c208c269e42d8abf69c465fbbc894",
              "IPY_MODEL_a03c734c22d04d1f99f74ea997c71ed0"
            ],
            "layout": "IPY_MODEL_76f23622354f4cb5b9e2f737003628fb"
          }
        },
        "1142f5c849954e6cac989c784e4a387f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16909b85ff6b4512b4b9ea88603b2f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21a3e7658b454c79b4779d66fe5c09ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c909cf22d1e4108b1eec8e07ec33381",
            "placeholder": "​",
            "style": "IPY_MODEL_961209945c2a463c9fad2c16b42b0966",
            "value": " 296/296 [00:00&lt;00:00, 19.8kB/s]"
          }
        },
        "236752401f18451a8a6f5c7b38eba5de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "257be2834e104dbba1d7de62b49697b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27aa0ca6db6f4364a9f574fa091e7dfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d0b9a59ec44be9b19dcce4c42e5ade": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3109dc8210a0462a9b6e2fef2e9c9e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76ee0a929f8947829b0d45dbd015155c",
              "IPY_MODEL_69e8d7a1010c490a962855e437171e65",
              "IPY_MODEL_02ed99de52864bc9b78351690dc13527"
            ],
            "layout": "IPY_MODEL_947130b7bd954befa0ed776f788a8655"
          }
        },
        "3770ca6ca30749c6bb7154bc0f38daff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_435ff2fd9de3419a873df70d05e74aec",
            "placeholder": "​",
            "style": "IPY_MODEL_940c426b56224f90af36fbcf6346c86e",
            "value": "generation_config.json: 100%"
          }
        },
        "389b5beef74d4b3fa1e6b52353a67edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa89b6252f714f3b845cfd0394f61720",
              "IPY_MODEL_8b9065a29a0842fe903ffcc31bbb035c",
              "IPY_MODEL_fe94cc6fc9c34833bfbe945a20034901"
            ],
            "layout": "IPY_MODEL_acb660264cbd4b1981ef130c231ab366"
          }
        },
        "39b0db9dd30f4509b200c07a9ab90c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f83ae0652cc94051a3b768ebe783b92d",
            "max": 2471645608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2aaf67964db42a8b40067a10c1631c0",
            "value": 2471645608
          }
        },
        "3a8721e429104fc9914397f6842561e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c909cf22d1e4108b1eec8e07ec33381": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "435ff2fd9de3419a873df70d05e74aec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44a48ae26fb241709d3599d3a2341ef7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47043cb15130426daeb100c76df2d8db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6bf15b421f94416b12f10367b66954d",
              "IPY_MODEL_9627a6184115419f9756bb65728ba1ae",
              "IPY_MODEL_c4799e68ecc14684825ff8021e1d1560"
            ],
            "layout": "IPY_MODEL_744d571bddff457995eee3517f478928"
          }
        },
        "4766c29b38d44c20a62182c3cbb120dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "478da38353374849b2bed6603945d0aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f56bd8d1b014ab5b8359d061752248e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5799a95d2ebf42b4973bee23ffa0ebd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd48a20babf4e8a9c082cefcdf5ece2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65db5147ff904ef2912cab82c499630b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66c2a2f9e9bd4935b3c8f02c5d30e885": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69e8d7a1010c490a962855e437171e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_236752401f18451a8a6f5c7b38eba5de",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00049c5d525042fc94d70ecb029f3ed0",
            "value": 9085657
          }
        },
        "744d571bddff457995eee3517f478928": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ee0a929f8947829b0d45dbd015155c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66c2a2f9e9bd4935b3c8f02c5d30e885",
            "placeholder": "​",
            "style": "IPY_MODEL_0c911ae2fc4a4d7493afdb3733b2c0ec",
            "value": "tokenizer.json: 100%"
          }
        },
        "76f23622354f4cb5b9e2f737003628fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6fa4339a984f479d032529cfebe9e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83de6611baf848d2af2c51c2a02f07e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b9065a29a0842fe903ffcc31bbb035c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1142f5c849954e6cac989c784e4a387f",
            "max": 54528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4766c29b38d44c20a62182c3cbb120dd",
            "value": 54528
          }
        },
        "8ee8053d0bf64d97874c656d22547c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "903cd418c29c4ecd897bcb59f4749b90": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "940c426b56224f90af36fbcf6346c86e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "947130b7bd954befa0ed776f788a8655": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "961209945c2a463c9fad2c16b42b0966": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9627a6184115419f9756bb65728ba1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5799a95d2ebf42b4973bee23ffa0ebd5",
            "max": 877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_478da38353374849b2bed6603945d0aa",
            "value": 877
          }
        },
        "98f6fad28154454fbc4d124207d85b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fe8c0ed12b241ce962e11a8e2699cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c25429eae7934ab8826c5ea7885cd68c",
              "IPY_MODEL_c40f1ded923f42c88639cfc72a4101df",
              "IPY_MODEL_21a3e7658b454c79b4779d66fe5c09ef"
            ],
            "layout": "IPY_MODEL_a3c1dc2560954d36980b51e7113ef744"
          }
        },
        "a03c734c22d04d1f99f74ea997c71ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa7f4caebabe49208917711765351c0e",
            "placeholder": "​",
            "style": "IPY_MODEL_c51cae176e7549dca08685de3c8d2f2a",
            "value": " 189/189 [00:00&lt;00:00, 16.0kB/s]"
          }
        },
        "a2aaf67964db42a8b40067a10c1631c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3c1dc2560954d36980b51e7113ef744": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6bf15b421f94416b12f10367b66954d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83de6611baf848d2af2c51c2a02f07e2",
            "placeholder": "​",
            "style": "IPY_MODEL_257be2834e104dbba1d7de62b49697b9",
            "value": "config.json: 100%"
          }
        },
        "acb660264cbd4b1981ef130c231ab366": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bae56bd372914f878da901dc1b6f9c1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29d0b9a59ec44be9b19dcce4c42e5ade",
            "placeholder": "​",
            "style": "IPY_MODEL_e51db02f108d4d17a172c49e027d1e7a",
            "value": " 2.47G/2.47G [00:57&lt;00:00, 47.8MB/s]"
          }
        },
        "beb5d97e3dac4f55b03011a6c34110a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c25429eae7934ab8826c5ea7885cd68c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f51d38e3ff314df69a33af09b067254c",
            "placeholder": "​",
            "style": "IPY_MODEL_d11d1071950843bab6148b1186b9a060",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "c40f1ded923f42c88639cfc72a4101df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_903cd418c29c4ecd897bcb59f4749b90",
            "max": 296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e915812763fc40ebbb982edffa48a9db",
            "value": 296
          }
        },
        "c4799e68ecc14684825ff8021e1d1560": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27aa0ca6db6f4364a9f574fa091e7dfa",
            "placeholder": "​",
            "style": "IPY_MODEL_d15750d5c58c4216ad7b13322d496ca7",
            "value": " 877/877 [00:00&lt;00:00, 46.2kB/s]"
          }
        },
        "c51cae176e7549dca08685de3c8d2f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5ea18f19d0c417aabeb4dcbc85dab2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edffe56813b24c6881307a397c758f12",
            "placeholder": "​",
            "style": "IPY_MODEL_8ee8053d0bf64d97874c656d22547c53",
            "value": "model.safetensors: 100%"
          }
        },
        "cd6c208c269e42d8abf69c465fbbc894": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beb5d97e3dac4f55b03011a6c34110a4",
            "max": 189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5dd48a20babf4e8a9c082cefcdf5ece2",
            "value": 189
          }
        },
        "d11d1071950843bab6148b1186b9a060": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d15750d5c58c4216ad7b13322d496ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e51db02f108d4d17a172c49e027d1e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e915812763fc40ebbb982edffa48a9db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eca3ee0e1dbc45c3a0ad0bf930a3b979": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5ea18f19d0c417aabeb4dcbc85dab2e",
              "IPY_MODEL_39b0db9dd30f4509b200c07a9ab90c7a",
              "IPY_MODEL_bae56bd372914f878da901dc1b6f9c1e"
            ],
            "layout": "IPY_MODEL_7b6fa4339a984f479d032529cfebe9e2"
          }
        },
        "edffe56813b24c6881307a397c758f12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f51d38e3ff314df69a33af09b067254c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83ae0652cc94051a3b768ebe783b92d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa7f4caebabe49208917711765351c0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa89b6252f714f3b845cfd0394f61720": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8721e429104fc9914397f6842561e1",
            "placeholder": "​",
            "style": "IPY_MODEL_65db5147ff904ef2912cab82c499630b",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fe94cc6fc9c34833bfbe945a20034901": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44a48ae26fb241709d3599d3a2341ef7",
            "placeholder": "​",
            "style": "IPY_MODEL_4f56bd8d1b014ab5b8359d061752248e",
            "value": " 54.5k/54.5k [00:00&lt;00:00, 3.39MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
